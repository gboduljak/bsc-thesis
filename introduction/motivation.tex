\section{Motivation}

For a long time, it was difficult to imagine that a computer could accurately classify and segment images, summarise or generate text or play strategic computer games at a superhuman level. It is interesting to note that most of the progress in those problems is driven by deep learning. Deep learning refers to a family of machine learning algorithms related to artificial neural networks. Artificial neural networks are a family of machine learning algorithms capable of learning functions by extracting increasingly complex hierarchical representations from the data. The name comes from a biological inspiration for their structure.
Mathematically, they are often nothing but a composition of nonlinear transformations of the input data. Those transformations are often initialized randomly and then learned from the data by some numerical optimization algorithm. Those transformations are often layered and parameterized. The computation of finding the optimal parameters for those transformations is called learning or training.

Although most of the computational problems mentioned above look seemingly unrelated, it turns out that they are all mathematically the same - they are all an instance of the problem of learning or approximating a (possibly) complex, an unknown function given data. Interestingly, the artificial neural networks excel at all mentioned tasks, often performing significantly better than different machine learning algorithms. Despite their impressive experimental performance, neural networks are often regarded as black-box models, due to the lack of theoretical guarantees and the difficulty of understanding their learning and decision-making process.

Given the recent success, it was natural to explore the mathematical properties of artificial neural networks and question their power of approximating functions. This thesis will focus on the simplest forms of neural networks - feed-forward, fully-connected neural networks. Despite their apparent simplicity, the rigorous analysis of the representation and approximation power of feed-forward neural networks turns out to be quite difficult, often involving various fields within mathematics, including general topology, measure theory, and functional analysis. Moreover, this is an open research problem, and papers addressing those issues are still published. You can read more about this in \nameref{chapter:literature-review}.

However, we have strong theoretical results regarding the approximation power of feed-forward neural networks. This project will tackle some of those in the increasing order of their complexity and generality. Although most of the results presented in this thesis are well-known in the machine learning community, they are often barely mentioned in machine learning textbooks and stated without proof or further rigorous discussion. A possible explanation of such a situation is the dependence on concepts and results from functional analysis, abstract measure theory, and general topology. Since the machine learning community is quite interdisciplinary, such demands on mathematical prerequisites are often out of the scope of those textbooks, aimed towards the more general audience. This thesis is an attempt to present the most fundamental results in approximation theory of neural networks assuming only undergraduate mathematics background. The necessary more advanced mathematical concepts are discussed in \nameref{chapter:appendix}. In this thesis, the focus is on important theoretical results and proofs, presented in \nameref{chapter:universality}. The relationship between established theoretical guarantees and practical performance of neural networks is discussed in \nameref{chapter:experiments}.