\subsection{Conclusion}

In this chapter, we discussed various practical problems related to the applications of neural networks. Despite impressive theoretical properties discussed in \nameref{chapter:literature-review} and \nameref{chapter:universality}, many practical applications of neural networks often suffer from issues not addressed in the universal approximation theory. 

According to state of the art results from  \nameref{chapter:literature-review}, $\operatorname{ReLU}$, $\operatorname{sigmoid}$ and $\operatorname{tanh}$ have comparable theoretical properties. However, in \nameref{subsection:experiments:classification:activation}, we observed that those activation functions result in noticeably different validation accuracy. This can be attributed to the little emphasis we put on the training configuration. This suggests that different activation functions may demand different training configurations. To address a mysterious performance gap between $\operatorname{ReLU}$ and alternatives,
more research is necessary. For example, studying the effects of weight regularisation (weight decay) or employing a more flexible learning rate scheduler could provide more insight into this observation. According to the results reported on the  \href{http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/}{Fashion MNIST benchmark dashboard}, a neural network with a single layer of 100 neurons and $\operatorname{ReLU}$ activation function achieved the accuracy of $87.7\%$. Moreover, such a network outperformed $\tanh$ counterparts. This may suggest that $\operatorname{ReLU}$ networks perform better than observed in this thesis.

We have also observed that the best validation accuracy peaks at about $82\%$.  According to the benchmark table in \cite{fashionmnistgithub}, more sophisticated convolutional neural networks achieve the validation accuracy exceeding $90\%$. This may indicate that the fully-connected architecture is generally too simple to perform the classification on this dataset.

Seemingly unimportant hyperparameter choices discussed in \nameref{subsection:experiments:classification:batch} and \nameref{subsection:experiments:classification:optimizer} are disconnected from the approximation theory of neural networks. However, we observed they may have significant practical consequences.